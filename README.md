# crawlers_Bloc
#### Crawlers désignent dans le monde de l'informatique un robot d'indexation   il s'agit d'un logiciel qui a pour principale mission d'explorer le Web afin d'analyser le contenu des documents visités et les stocker de manière organisée dans un index.

Alors voici un script qui permet de bloquer les scrawlers à l'aide des commandes de base linux dont : 
    <h5>♠ awk </h5> 
    
<h5>♠ tail </h5> 
    
<h5>♠ iptables </h5> 
    
<h5>♠ sed </h5> 
    
<h5>♠ crontab</h5> 
   
Voici le lien du script :
<a href="https://github.com/Mitsanta12/crawlers_Block/blob/main/crawlersBlock.sh">CRAWLERS_BLOCK</a>
